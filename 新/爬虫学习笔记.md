# 爬虫的学习笔记:book:



## 对爬虫的一些查询内容

+ 什么是爬虫

  网络爬虫，是一种按照**一定规则，自动**抓取互联网信息的程序或脚本.

  由于互联网数据的多样性和资源的有限性，根据**用户需求**定向抓取相关

  网页并分析已成为如今主流的爬取策略

+ 爬虫可以做什么

  可以爬取文本，图片，音频，视频。只要能通过浏览器访问的数据基本都可以

  通过爬虫获取。

+ 爬虫的本质是什么

  **模拟浏览器**打开网页，获取网页中我们想要的那部分数据。





## 爬虫的基本流程

+ 准备工作

  通过浏览器查看分析目标网页.

  

+ 获取数据

  通过HTTP库想目标站点发送请求，请求可以包含额外的header

  等信息，如果服务器能正常响应，会得到一个Response，便是

  所要获取的页面内容

  **其中request-method有Post与Get两种**

  

+ 解析内容

  得到内容可能是HTML，json等格式，可以通过页面解析库、正则

  表达式等进行解析。

  

+ 保存数据

  保存可为文本，也可保存到数据库，或者特定的格式文件。



### 获取数据

1. 对需要爬取的网页定义一个函数URL获取页面内容

   

2. 传入需要的url参数表示网址

   

3. urllib.request生成请求，向服务器发送

   

4. 在访问页面时经常会出现错误，为了程序正常运行，加入try……except语句

   

**需要注意的时一些网站有反爬能力，我们需要打开浏览器的开发者模式，
获取一些参数，用以伪装成浏览器以成功得到服务器响应**



### 对爬取的HTML文件进行解析



1. 使用BeautifulSoup定位特定的标签位置

   

2. 使用正则表达式找到具体的内容





### BeautifulfSoup库的使用方法

`bs=BeautifulfSoup(html，'html.parser')`



其中html是所要解析的对象

其中'html.parser'是解析器



| 例子                                  | 效果                                   |
| ------------------------------------- | -------------------------------------- |
| bs.tittle                             | 标签及其内容：只取第一个               |
| bs.tittle.string                      | 标签里的内容（字符串）                 |
| bs.tittle.attrs                       | 标签里的属性                           |
| bs.find_all("a")                      | 字符串过滤：查找于字符串完全匹配的内容 |
| bs.find_all(re.compile("正则表达式")) | 结合正则表达式                         |
| bs.head.contents                      | 获取HTML的head中的content标签          |
|                                       |                                        |



### Re库主要功能函数

| 函数           | 说明                                                         |
| -------------- | ------------------------------------------------------------ |
| re.search( )   | 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象  |
| re.match( )    | 从一个字符串的开始位置起匹配正则表达式，返回match对象        |
| re.findall( )  | 搜索字符串，以列表类型返回全部能匹配的子串                   |
| re.split( )    | 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型     |
| re.finditer( ) | 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match的对象 |
| re.sub( )      | 在一个字符串中替换所有匹配正则表达式的字串，返回替换后的字符串 |



### 正则表达式常用操作符

| 操作符 | 说明                             | 实例                                   |
| ------ | -------------------------------- | -------------------------------------- |
| .      | 表示任何单个字符                 |                                        |
| [ ]    | 字符集，对单个字符给出取值范围   | [abc]表示a、b、c,[a-z]表示a到z单个字符 |
| [^ ]   | 非字符集，对单个字符给出排除范围 | [^abc ]表示非a或b或c的单个字符         |
| *      | 前一个字符0次或无限次扩展        | abc*表示ab、abc、abcc、abccc等         |
| +      | 前一个字符1次或无限次扩展        | abc+表示abc、abcc、abccc等             |
| ?      | 前一个字符0次或1次扩展           | abc? 表示ab、abc                       |
| \|     | 左右表达式任意一个               | abc\|def表示abc、def                   |
| {m}    | 拓展前一个字符m次                | ab{2}c表示abbc                         |
| {m，n} | 拓展前一个字符m-n次（含n）       | ab{1,2}c表示abc、abbc                  |
| ^      | 匹配字符串开头                   | ^abc 表示abc且在一个字符串的开头       |
| ( )    | 分组标记，内部只能使用\|操作符   | (abc)表示abc,  (abc\|def)表示abc、def  |
| `$`    | 匹配字符串结尾                   | abc`$`表示abc且在一个字符串的结尾      |
| \d     | 数字，等价于[0-9]                |                                        |
| \w     | 单词字符，等价于[A-Za-z0-9]      |                                        |



### 一些基本的网页反爬机制

+ 通过UA机制识别爬虫

  UA的全称是User Agent，它是请求浏览器的身份标志，很多网站使用

  它来作为识别爬虫的标志，如果访问请求的头部中没有带UA那么就会

  被判定为爬虫，但由于这种要针对这种反爬虫机制十分容易，即随机UA，

  所以这种反爬机制使用的很少 

  

+ 通过访问频率识别爬虫

  爬虫为了保证效率，往往会在很短的时间内多次访问目标网站，所以可

  以通过单个IP访问的频率来判断是否为爬虫。并且，这种反爬方式比较

  难以被反反爬机制反制，只能通过更换代理IP来保证效率

  

+ 通过Cookie和验证码识别爬虫

  Cookie就是指会员制的账号密码登陆验证，这就可以通过限制单账号抓

  取频率来限制爬虫抓取，而验证码完全是随机的，爬虫脚本无法正确识别，

  同样可以限制爬虫程序
