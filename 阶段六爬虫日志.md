# 阶段六爬虫日志



### 开始，为了体验爬取网页视频我先粗略地

### 仿照教程试了一下爬取微博的视频：



[这是视频原始网址](https://weibo.com/tv/show/1034:4725864357888081?mid=4725866133196408)  此时的method为Get.



![微博例子1.png](https://github.com/itawenya/Tasks/blob/main/%E6%96%B0/%E5%BE%AE%E5%8D%9A%E4%BE%8B%E5%AD%901.png?raw=true)

在Chorm中打开 开发者模式中network的media

选取Request Url网址中的一小段并在左侧搜索框中搜索

![微博例子2.png](https://github.com/itawenya/Tasks/blob/main/%E6%96%B0/%E5%BE%AE%E5%8D%9A%E4%BE%8B%E5%AD%902.png?raw=true)





选中第一项中有component的内容

在这里显示的Request Url网址即[播放视频的网址](https://weibo.com/ajax/changekey)  此时的method为Post



![微博例子3.png](https://github.com/itawenya/Tasks/blob/main/%E6%96%B0/%E5%BE%AE%E5%8D%9A%E4%BE%8B%E5%AD%903.png?raw=true)



在preview中可以看到视频链接按照一下层级顺序包含：

```
['data']['Component_Play_Playinfo']['urls']['高清 1080P']
```



![微博例子4.png](https://github.com/itawenya/Tasks/blob/main/%E6%96%B0/%E5%BE%AE%E5%8D%9A%E4%BE%8B%E5%AD%904.png?raw=true)



接着我们需要搜索一些必要参数用以伪装

比如：cookie，referer，user-agent和data



cookie，referer，user-agent可以在初始网址找到



**我们需要在【播放视频的网址】的playload里找到data，但让我不解的是同样的方法有的视频我却找不到data**

![微博例子6.png](https://github.com/itawenya/Tasks/blob/main/%E6%96%B0/%E5%BE%AE%E5%8D%9A%E4%BE%8B%E5%AD%906.png?raw=true)





附上我体验爬取微博视频的源码：

```
import requests
url = 'https://weibo.com/tv/api/component?page=%2Ftv%2Fshow%2F1034%3A4725864357888081'
data = {'data': '{"Component_Play_Playinfo":{"oid":"1034:4725864357888081"}}'}
headers = {
    'cookie': 'SUB=_2AkMWuJ0Gf8NxqwJRmP0dz2rjZY5yygjEieKg5GzdJRMxHRl-yT9jqmoItRB6PTiz6YRoPNuQmhnFjCsRR3eR7yuHqwKD; SUBP=0033WrSXqPxfM72-Ws9jqgMF55529P9D9WFpzJ55kj15F0Txa3TCNWE.; SINAGLOBAL=3400756052629.177.1642336817708; TC-V-WEIBO-G0=b09171a17b2b5a470c42e2f713edace0; _s_tentry=-; Apache=9992151456252.889.1642641806570; ULV=1642641806648:3:3:3:9992151456252.889.1642641806570:1642488693713; XSRF-TOKEN=fI_JWAxlLT4TaBSe9lVP_Lqf; WBPSESS=a_YZA6I5qCR3U8i3Rfvlpp4O4vRhTMjBcq8WcoQvWqSOLzgAqPD_v48xdKHPDw2gtPIZhqAoMa314yTQfllHxTc0qWssWIiWQ6WMZS8yv2EYae1hx2MpXxGJb5m4q4tt5swZWQ1i0PJO89JXmK_57EAKnS3QD_pEHzrIxYlVr_I=',
    'referer': 'https://weibo.com/tv/show/1034:4725864357888081?mid=4725866133196408',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'
}


json_data = requests.post(url=url,data=data,headers=headers).json()
video_url = 'https:' + json_data['data']['Component_Play_Playinfo']['urls']['高清 1080P']
video_data = requests.get(video_url).content
with open('video\\1.MP4','wb')as f:
    f.write(video_data)
print('视频爬取好啦！')
```

成功！



#### 一开始的尝试让我体验到了爬虫的乐趣，但还没有学习具体的知识

#### 接下来我跟着视频教程对爬虫技术进行了学习。







### 我学习的视频的例子是爬取豆瓣TOP250的信息，为了简化

### 我决定只取第一页，省去了翻页的循环



这里附上笔记链接🔗：[Tasks/爬虫学习笔记.md at main · itawenya/Tasks (github.com)](https://github.com/itawenya/Tasks/blob/main/新/爬虫学习笔记.md)



### 豆瓣项目实操起来好像没问题，但当我把url改成我们[Geek的网址](http://212.129.245.115/geek)时

### 却出现了问题:

当我试着输出网页的源代码时

while豆瓣网址能输出完整的代码

![发现问题1.png](https://github.com/itawenya/Tasks/blob/main/%E6%96%B0/%E5%8F%91%E7%8E%B0%E9%97%AE%E9%A2%981.png?raw=true)





而Geek网址却只能输出部分的代码

![发现问题2.png](https://github.com/itawenya/Tasks/blob/main/%E6%96%B0/%E5%8F%91%E7%8E%B0%E9%97%AE%E9%A2%982.png?raw=true)





附上此时的源码：

```python
import urllib.request
import bs4
url = "https://movie.douban.com/top250?start="
#url = "http://212.129.245.115/geek"

def askURL(url): #模拟浏览器头部信息，进行伪装
    head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'}
    request = urllib.request.Request(url, headers=head)
    html = ""
    try:
        response = urllib.request.urlopen(request)
        html = response.read().decode('utf-8')
        print(html)
    except urllib.error.URLError as e:
        if hasattr(e, 'code'):
            print(e.code)
        if hasattr(e, 'reason'):
            print(e.reason)
    return html

askURL(url)
```



### 看起来就像是输出的结果被折叠起来了

### 但起初我没有在意







### 但当我学了Re库和正则表达式后想输出Geek网址的

### body内容时才意识到问题



这是豆瓣的：

![发现问题3.png](https://github.com/itawenya/Tasks/blob/main/%E6%96%B0/%E5%8F%91%E7%8E%B0%E9%97%AE%E9%A2%983.png?raw=true)



这是Geek的：只有细长的一条

![发现问题4.png](https://github.com/itawenya/Tasks/blob/main/%E6%96%B0/%E5%8F%91%E7%8E%B0%E9%97%AE%E9%A2%984.png?raw=true)





### 为了查找原因，我做了以下尝试

#### 1.我首先尝试了几个不同的网址并运行，

#### 发现都可以像豆瓣一样展示  

####  → 证明我的代码可行





#### 2.接着我查看每个对应网址的开发者工具我

#### 发现此时输出的内容正是Response中的内容

#### 而Geek网址的本身就是没有展开  

#### → 发现潜在原因





### 我得出以下猜想：（当然啊不一定对）
#### 1.也许与网址的反爬机制有关



#### 2.也许与JavaScript，CSS链接有关





### 我有些受挫，前边学习了解技术的时间貌似有些长了

### 我留给操作Geek网址的时间不多了

（我以为只是改变网址的区别而已）





#### 但我发现Geek图片的网址都来源与一个地方

→https://gitee.com/gritandsea/picture/tree/master/img

#### 也许我可以试着爬取这个网址的图片？

这里附上源码：

```
import re
import urllib.request
import bs4
url = 'https://gitee.com/gritandsea/picture/tree/master/img'

def main():
    data = getData(url)
    askURL(url)

# findlink = re.compile(r'<a title=(.*>?)>')  【这是一个失败的尝试】
findlink = re.compile(r'<a href="(.*?)">')   【我试着把a标签带链接的输出】

def getData(url): #爬取网址
    data = []
    html = askURL(url)
    soup = bs4.BeautifulSoup(html, "html.parser")
    for item in soup.find_all('a'):
    	item = str(item)
        link = re.findall(findlink,item)
        print(link)
        data.append(link)
        c = str(data)
    
    print(data)
    with open('img\\pic-link.text', 'w')as f:
        f.write(c)

    print('写完了')
    return data
    

def askURL(url): #模拟浏览器头部信息，进行伪装
    head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'}
    request = urllib.request.Request(url, headers=head)
    html = ""
    try:
        response = urllib.request.urlopen(request)
        html = response.read().decode('utf-8')
        #print(html)
    except urllib.error.URLError as e:
        if hasattr(e, 'code'):
            print(e.code)
        if hasattr(e, 'reason'):
            print(e.reason)
    return html

def saveData(savepath):  #保存数据
    print('save....')

if __name__ == "__main__":
    main()
```



感觉是正则提取法或者哪个没有检查出

的问题的原因，可以看到输出结果差强人意：虽然都包含了图片链接但还有很多杂项



![输出效果.png](https://github.com/itawenya/Tasks/blob/main/%E6%96%B0/%E8%BE%93%E5%87%BA%E6%95%88%E6%9E%9C.png?raw=true)





### 时间不够了，老实说阶段六任务没有很好地完成

### 这段时间的学习让我学到

+ 不同阶段的学习内容紧密联系，
+ 一定要多尝试敲代码，不要只看教程
+ 即使看代码能读个大概，例子也基本能完成也一定要自己多多尝试

+ 本次爬虫任务不仅需要清晰的代码逻辑，第一次写长逻辑链的代码有点

​       不熟练



### 体验过后我认为爬虫重点是在结合HTML的前端知识，利用正则法制和beautifulsoup

### 靓汤来准确查找所需的内容的条件。 还需要继续学习啊！！！

 



​                                                                                                                                                   ----wy
